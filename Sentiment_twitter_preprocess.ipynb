{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0496d9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as nlp\n",
    "tweets=pd.read_csv('./data/twitter_sentiment_noemoticon.csv',encoding='latin', \n",
    "                   names = ['sentiment','id','date','query','user','tweet'])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee561b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 6)\n"
     ]
    }
   ],
   "source": [
    "# sample a small debug dataset\n",
    "data = tweets.sample(n=50000)\n",
    "print(\"Dataset shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3719b5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbcfdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1355859</th>\n",
       "      <td>1</td>\n",
       "      <td>2047570063</td>\n",
       "      <td>Fri Jun 05 13:51:02 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JillCarlier</td>\n",
       "      <td>has trouble getting any chores done when the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259633</th>\n",
       "      <td>1</td>\n",
       "      <td>1998148839</td>\n",
       "      <td>Mon Jun 01 18:04:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JazziCouture</td>\n",
       "      <td>Ima have me a few drinks...  Hope my rude ass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194489</th>\n",
       "      <td>0</td>\n",
       "      <td>1970300908</td>\n",
       "      <td>Sat May 30 03:17:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OvErLoRD3991</td>\n",
       "      <td>@whatithinke http://twitpic.com/68qmw - lol, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118134</th>\n",
       "      <td>0</td>\n",
       "      <td>1827771302</td>\n",
       "      <td>Sun May 17 11:24:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Trace027</td>\n",
       "      <td>@CocaBeenSlinky no its a tooth i got root cana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871244</th>\n",
       "      <td>1</td>\n",
       "      <td>1678604815</td>\n",
       "      <td>Sat May 02 07:13:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>M_G1</td>\n",
       "      <td>@misty glad you are getting some peace later t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778078</th>\n",
       "      <td>0</td>\n",
       "      <td>2322583565</td>\n",
       "      <td>Wed Jun 24 22:52:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AlyssaRachelleW</td>\n",
       "      <td>so yea going back to colorado to pack my stuff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366480</th>\n",
       "      <td>1</td>\n",
       "      <td>2050191789</td>\n",
       "      <td>Fri Jun 05 18:13:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CodyJoee</td>\n",
       "      <td>is excited for SCUBA classes in July!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485793</th>\n",
       "      <td>1</td>\n",
       "      <td>2067941805</td>\n",
       "      <td>Sun Jun 07 13:13:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PODgirl_</td>\n",
       "      <td>@memset Ð¿ÑÐ¸Ð²ÐµÐ·Ð¸ Ð¼Ð½Ðµ Ð²ÐºÑÑ?Ð½Ð¾Ð³Ð¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708947</th>\n",
       "      <td>0</td>\n",
       "      <td>2257461233</td>\n",
       "      <td>Sat Jun 20 14:22:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>amythefitz</td>\n",
       "      <td>@sedser Aww really?? That sucks..I found it so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751650</th>\n",
       "      <td>0</td>\n",
       "      <td>2286181246</td>\n",
       "      <td>Mon Jun 22 16:06:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dianeblondiec</td>\n",
       "      <td>@SamanthaBolland nope  lots of diana connors, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "1355859          1  2047570063  Fri Jun 05 13:51:02 PDT 2009  NO_QUERY   \n",
       "1259633          1  1998148839  Mon Jun 01 18:04:23 PDT 2009  NO_QUERY   \n",
       "194489           0  1970300908  Sat May 30 03:17:29 PDT 2009  NO_QUERY   \n",
       "118134           0  1827771302  Sun May 17 11:24:46 PDT 2009  NO_QUERY   \n",
       "871244           1  1678604815  Sat May 02 07:13:01 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "778078           0  2322583565  Wed Jun 24 22:52:55 PDT 2009  NO_QUERY   \n",
       "1366480          1  2050191789  Fri Jun 05 18:13:52 PDT 2009  NO_QUERY   \n",
       "1485793          1  2067941805  Sun Jun 07 13:13:25 PDT 2009  NO_QUERY   \n",
       "708947           0  2257461233  Sat Jun 20 14:22:25 PDT 2009  NO_QUERY   \n",
       "751650           0  2286181246  Mon Jun 22 16:06:07 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1355859      JillCarlier  has trouble getting any chores done when the w...  \n",
       "1259633     JazziCouture  Ima have me a few drinks...  Hope my rude ass ...  \n",
       "194489      OvErLoRD3991  @whatithinke http://twitpic.com/68qmw - lol, t...  \n",
       "118134          Trace027  @CocaBeenSlinky no its a tooth i got root cana...  \n",
       "871244              M_G1  @misty glad you are getting some peace later t...  \n",
       "...                  ...                                                ...  \n",
       "778078   AlyssaRachelleW  so yea going back to colorado to pack my stuff...  \n",
       "1366480         CodyJoee             is excited for SCUBA classes in July!   \n",
       "1485793         PODgirl_  @memset Ð¿ÑÐ¸Ð²ÐµÐ·Ð¸ Ð¼Ð½Ðµ Ð²ÐºÑÑ?Ð½Ð¾Ð³Ð¾...  \n",
       "708947        amythefitz  @sedser Aww really?? That sucks..I found it so...  \n",
       "751650     dianeblondiec  @SamanthaBolland nope  lots of diana connors, ...  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace sentiment values to 0 (negative) or 1 (positive)\n",
    "data['sentiment']=data['sentiment'].replace(4,1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "354f4292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1355859</th>\n",
       "      <td>1</td>\n",
       "      <td>has trouble getting any chores done when the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259633</th>\n",
       "      <td>1</td>\n",
       "      <td>Ima have me a few drinks...  Hope my rude ass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194489</th>\n",
       "      <td>0</td>\n",
       "      <td>@whatithinke http://twitpic.com/68qmw - lol, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118134</th>\n",
       "      <td>0</td>\n",
       "      <td>@CocaBeenSlinky no its a tooth i got root cana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871244</th>\n",
       "      <td>1</td>\n",
       "      <td>@misty glad you are getting some peace later t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778078</th>\n",
       "      <td>0</td>\n",
       "      <td>so yea going back to colorado to pack my stuff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366480</th>\n",
       "      <td>1</td>\n",
       "      <td>is excited for SCUBA classes in July!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485793</th>\n",
       "      <td>1</td>\n",
       "      <td>@memset Ð¿ÑÐ¸Ð²ÐµÐ·Ð¸ Ð¼Ð½Ðµ Ð²ÐºÑÑ?Ð½Ð¾Ð³Ð¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708947</th>\n",
       "      <td>0</td>\n",
       "      <td>@sedser Aww really?? That sucks..I found it so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751650</th>\n",
       "      <td>0</td>\n",
       "      <td>@SamanthaBolland nope  lots of diana connors, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet\n",
       "1355859          1  has trouble getting any chores done when the w...\n",
       "1259633          1  Ima have me a few drinks...  Hope my rude ass ...\n",
       "194489           0  @whatithinke http://twitpic.com/68qmw - lol, t...\n",
       "118134           0  @CocaBeenSlinky no its a tooth i got root cana...\n",
       "871244           1  @misty glad you are getting some peace later t...\n",
       "...            ...                                                ...\n",
       "778078           0  so yea going back to colorado to pack my stuff...\n",
       "1366480          1             is excited for SCUBA classes in July! \n",
       "1485793          1  @memset Ð¿ÑÐ¸Ð²ÐµÐ·Ð¸ Ð¼Ð½Ðµ Ð²ÐºÑÑ?Ð½Ð¾Ð³Ð¾...\n",
       "708947           0  @sedser Aww really?? That sucks..I found it so...\n",
       "751650           0  @SamanthaBolland nope  lots of diana connors, ...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unused features\n",
    "data.drop(['date','query','user'], axis=1, inplace=True)\n",
    "data.drop('id', axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb709927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "tweet        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50685aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af7b765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data is:         50000\n",
      "No. of positve tagged sentences is:  25024\n",
      "No. of negative tagged sentences is: 24976\n"
     ]
    }
   ],
   "source": [
    "positives = data['sentiment'][data.sentiment == 1 ]\n",
    "negatives = data['sentiment'][data.sentiment == 0 ]\n",
    "\n",
    "print('Total length of the data is:         {}'.format(data.shape[0]))\n",
    "print('No. of positve tagged sentences is:  {}'.format(len(positives)))\n",
    "print('No. of negative tagged sentences is: {}'.format(len(negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f9a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"hadn't\", 'herself', 'most', 'both', 'at', 'i', 'their', \"needn't\", \"aren't\", 'themselves', 'couldn', 'than', 'm', 'an', 'through', 'few', 'some', 'whom', 'how', 'ma', 'from', 'himself', 'very', 've', 'has', 'she', 'what', 'with', 'between', 'mightn', 'ours', 'out', \"couldn't\", 'off', 'needn', 'as', \"should've\", 'yourself', 'have', 're', 'so', 'isn', 'your', 'it', 'them', 'only', 'who', \"you're\", 'been', 'against', 'by', \"you've\", 'same', \"wouldn't\", 'll', 'they', 'there', 'now', 'these', 'each', 'that', 'wouldn', 'him', \"won't\", 'own', \"you'll\", 'not', 'over', 'weren', 'does', 'being', 'yourselves', 'where', 'me', 's', 'hasn', 'itself', \"doesn't\", 'doesn', \"wasn't\", 'did', 'during', 'when', 'do', 't', 'if', 'up', 'his', 'wasn', 'shan', 'then', 'until', 'y', 'other', 'hadn', 'nor', 'because', 'her', 'yours', 'can', \"it's\", 'above', 'had', 'below', 'ourselves', 'd', 'why', 'this', \"isn't\", 'again', 'no', \"weren't\", 'here', \"that'll\", 'will', \"she's\", 'theirs', 'before', 'having', 'about', 'just', 'more', 'our', 'were', 'my', 'was', 'hers', 'such', 'or', 'and', 'ain', 'is', 'won', 'myself', 'the', 'don', 'down', 'o', 'he', 'on', 'are', 'while', \"mustn't\", \"you'd\", 'which', 'further', 'any', 'too', \"don't\", 'doing', 'a', 'those', 'to', 'into', 'but', 'haven', 'didn', \"mightn't\", 'should', 'mustn', \"shan't\", \"shouldn't\", \"didn't\", \"hasn't\", 'am', 'for', 'be', \"haven't\", 'aren', 'once', 'in', 'we', 'under', 'of', 'all', 'you', 'its', 'shouldn', 'after'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ray_f\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ray_f\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ray_f\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) \n",
    "#that a search engine has been programmed to ignore,\n",
    "#both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9307e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "bad = 'amp,today,tomorrow,going,girl,aed,oed,eed' # these words might fail in nltk stemmer algorithm\n",
    "def process_tweets(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)  \n",
    "    \n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = re.sub(r\"yrs\", \"years\", tweet)\n",
    "    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n",
    "    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n",
    "    tweet = re.sub(r\"2day\", \"today\", tweet)\n",
    "    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n",
    "    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n",
    "    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n",
    "    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n",
    "    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n",
    "    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n",
    "    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n",
    "    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n",
    "    tweet = re.sub(r\"goood\", \"good\", tweet)\n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "#     tweet=tweet[1:]\n",
    "    # Removing all URls \n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet) \n",
    "    #remove bad words\n",
    "    tweet= re.sub(bad,'',tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #tokens = [w for w in tokens if len(w)>2]\n",
    "    #Removing Stop Words\n",
    "    final_tokens = [w for w in tokens if w not in stopword]\n",
    "    #reducing a word to its word stem \n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "        if len(w)>1:\n",
    "            word = wordLemm.lemmatize(w)\n",
    "            finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a68baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", \n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "     \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}\n",
    "def convert_abbrev_in_text(tweet):\n",
    "    t=[]\n",
    "    words=tweet.split()\n",
    "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
    "    return ' '.join(t)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efcd4940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1355859</th>\n",
       "      <td>1</td>\n",
       "      <td>has trouble getting any chores done when the w...</td>\n",
       "      <td>trouble getting chore done weather nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259633</th>\n",
       "      <td>1</td>\n",
       "      <td>Ima have me a few drinks...  Hope my rude ass ...</td>\n",
       "      <td>ima drink hope rude as dont say something outt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194489</th>\n",
       "      <td>0</td>\n",
       "      <td>@whatithinke http://twitpic.com/68qmw - lol, t...</td>\n",
       "      <td>laughing out loud come blogtv bad still deep c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118134</th>\n",
       "      <td>0</td>\n",
       "      <td>@CocaBeenSlinky no its a tooth i got root cana...</td>\n",
       "      <td>tooth got root canal done year ago ever since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871244</th>\n",
       "      <td>1</td>\n",
       "      <td>@misty glad you are getting some peace later t...</td>\n",
       "      <td>glad getting peace later though hey keyboard f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778078</th>\n",
       "      <td>0</td>\n",
       "      <td>so yea going back to colorado to pack my stuff...</td>\n",
       "      <td>yea going back colorado pack stuff miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366480</th>\n",
       "      <td>1</td>\n",
       "      <td>is excited for SCUBA classes in July!</td>\n",
       "      <td>excited scuba class july</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485793</th>\n",
       "      <td>1</td>\n",
       "      <td>@memset Ð¿ÑÐ¸Ð²ÐµÐ·Ð¸ Ð¼Ð½Ðµ Ð²ÐºÑÑ?Ð½Ð¾Ð³Ð¾...</td>\n",
       "      <td>ð¿ñð¸ð²ðµð·ð¸ ð¼ð½ðµ ð²ðºññð½ð¾ð³ð¾ ðð°ð ñ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708947</th>\n",
       "      <td>0</td>\n",
       "      <td>@sedser Aww really?? That sucks..I found it so...</td>\n",
       "      <td>aww really sucksi found soi wondering whether ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751650</th>\n",
       "      <td>0</td>\n",
       "      <td>@SamanthaBolland nope  lots of diana connors, ...</td>\n",
       "      <td>nope lot diana connors diane oconnors strrange...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet  \\\n",
       "1355859          1  has trouble getting any chores done when the w...   \n",
       "1259633          1  Ima have me a few drinks...  Hope my rude ass ...   \n",
       "194489           0  @whatithinke http://twitpic.com/68qmw - lol, t...   \n",
       "118134           0  @CocaBeenSlinky no its a tooth i got root cana...   \n",
       "871244           1  @misty glad you are getting some peace later t...   \n",
       "...            ...                                                ...   \n",
       "778078           0  so yea going back to colorado to pack my stuff...   \n",
       "1366480          1             is excited for SCUBA classes in July!    \n",
       "1485793          1  @memset Ð¿ÑÐ¸Ð²ÐµÐ·Ð¸ Ð¼Ð½Ðµ Ð²ÐºÑÑ?Ð½Ð¾Ð³Ð¾...   \n",
       "708947           0  @sedser Aww really?? That sucks..I found it so...   \n",
       "751650           0  @SamanthaBolland nope  lots of diana connors, ...   \n",
       "\n",
       "                                          processed_tweets  \n",
       "1355859            trouble getting chore done weather nice  \n",
       "1259633  ima drink hope rude as dont say something outt...  \n",
       "194489   laughing out loud come blogtv bad still deep c...  \n",
       "118134   tooth got root canal done year ago ever since ...  \n",
       "871244   glad getting peace later though hey keyboard f...  \n",
       "...                                                    ...  \n",
       "778078             yea going back colorado pack stuff miss  \n",
       "1366480                           excited scuba class july  \n",
       "1485793  ð¿ñð¸ð²ðµð·ð¸ ð¼ð½ðµ ð²ðºññð½ð¾ð³ð¾ ðð°ð ñ...  \n",
       "708947   aww really sucksi found soi wondering whether ...  \n",
       "751650   nope lot diana connors diane oconnors strrange...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['processed_tweets'] = data['tweet'].apply(lambda x: process_tweets(x))\n",
    "data['processed_tweets'] = data['processed_tweets'].apply(lambda x: convert_abbrev_in_text(x))\n",
    "print('Text Preprocessing complete.')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54eecc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08ce36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe11452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.20,random_state=0)\n",
    "train_data = train_data.reset_index()\n",
    "test_data = test_data.reset_index()\n",
    "X_train=train_data['processed_tweets']\n",
    "y_train=train_data['sentiment']\n",
    "X_test=test_data['processed_tweets']\n",
    "y_test=test_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6477fff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  hi ho hi ho pay bill go\n",
       "1        premiere showing first ever episode doctor hdd...\n",
       "2        tell doorsalesmen get hell property instead bu...\n",
       "3        yeah jus really confused cuz miss rhea din dow...\n",
       "4                                  really massive headache\n",
       "                               ...                        \n",
       "39995    hahaha oh silly little strawberry making silly...\n",
       "39996                               always network timeout\n",
       "39997                 wheres best place buy laptop country\n",
       "39998                                thanks fun last night\n",
       "39999                                    ha tumbld weekend\n",
       "Name: processed_tweets, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9190c4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "39995    1\n",
       "39996    0\n",
       "39997    0\n",
       "39998    1\n",
       "39999    1\n",
       "Name: sentiment, Length: 40000, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf4e32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def print_paramter_vals(laplace, pos_prior):\n",
    "    print(f\"Unigram Laplace {laplace}\")\n",
    "    print(f\"Positive prior {pos_prior}\")\n",
    "\n",
    "\n",
    "def generateWordDict(train_set, train_labels, isPos):\n",
    "    wordDict = {}\n",
    "    for i in range(len(train_set)):\n",
    "        sentence = train_set[i]\n",
    "        if (train_labels[i] != isPos):\n",
    "            continue\n",
    "        for word in sentence:\n",
    "            if (word in wordDict):\n",
    "                wordDict[word] = wordDict[word] + 1\n",
    "            else:\n",
    "                wordDict[word] = 1\n",
    "    return wordDict\n",
    "\n",
    "\n",
    "def calculateProb(wordMap, laplace):\n",
    "    probMap = {}\n",
    "    n_type = len(wordMap)\n",
    "    n_token = 0\n",
    "    for word in wordMap:\n",
    "        n_token += wordMap[word]\n",
    "\n",
    "    for word in wordMap:\n",
    "        prob = (wordMap[word] + laplace)/(n_token + laplace*(n_type+1))\n",
    "        probMap[word] = prob\n",
    "\n",
    "    unk_prob = laplace/(n_token + laplace*(n_type+1))\n",
    "\n",
    "    return probMap, unk_prob\n",
    "\n",
    "def naiveBayes(train_set, train_labels, dev_set, laplace=0.01, pos_prior=0.5, silently=False):\n",
    "    print_paramter_vals(laplace, pos_prior)\n",
    "    positiveWordMap = generateWordDict(train_set, train_labels, 1)\n",
    "    negativeWordMap = generateWordDict(train_set, train_labels, 0)\n",
    "\n",
    "    positiveProbMap, positive_unk_prob = calculateProb(positiveWordMap, laplace)\n",
    "    negativeProbMap, negative_unk_prob = calculateProb(negativeWordMap, laplace)\n",
    "\n",
    "    yhats = []\n",
    "    for doc in tqdm(dev_set, disable=silently):\n",
    "        pos_prob = math.log(pos_prior)\n",
    "        neg_prob = math.log(1-pos_prior)\n",
    "\n",
    "        for word in doc:\n",
    "            if word in positiveProbMap:\n",
    "                pos_prob += math.log(positiveProbMap[word])\n",
    "            else:\n",
    "                pos_prob += math.log(positive_unk_prob)\n",
    "\n",
    "            if word in negativeProbMap:\n",
    "                neg_prob += math.log(negativeProbMap[word])\n",
    "            else:\n",
    "                neg_prob += math.log(negative_unk_prob)\n",
    "\n",
    "        if (pos_prob >= neg_prob):\n",
    "            yhats.append(1)\n",
    "        else:\n",
    "            yhats.append(0)\n",
    "    return yhats\n",
    "\n",
    "\n",
    "def print_paramter_vals_bigram(unigram_laplace, bigram_laplace, bigram_lambda, pos_prior):\n",
    "    print(f\"Unigram Laplace {unigram_laplace}\")\n",
    "    print(f\"Bigram Laplace {bigram_laplace}\")\n",
    "    print(f\"Bigram Lambda {bigram_lambda}\")\n",
    "    print(f\"Positive prior {pos_prior}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "You can modify the default values for the Laplace smoothing parameters, model-mixture lambda parameter, and the prior for the positive label.\n",
    "Notice that we may pass in specific values for these parameters during our testing.\n",
    "\"\"\"\n",
    "\n",
    "# main function for the bigrammixture model\n",
    "\n",
    "def generateBigramDict(train_set, train_labels, isPos):\n",
    "    bigramDict = {}\n",
    "    for i in range(len(train_labels)):\n",
    "        sentence = train_set[i]\n",
    "        if (train_labels[i] != isPos):\n",
    "            continue\n",
    "        for j in range(len(sentence)-1):\n",
    "            bigram = (sentence[j], sentence[j+1])\n",
    "\n",
    "            if (bigram in bigramDict):\n",
    "                bigramDict[bigram] = bigramDict[bigram] + 1\n",
    "            else:\n",
    "                bigramDict[bigram] = 1\n",
    "    return bigramDict\n",
    "\n",
    "def bigramBayes(train_set, train_labels, dev_set, unigram_laplace=0.001, bigram_laplace=0.005, bigram_lambda=0.5, pos_prior=0.5, silently=False):\n",
    "    print_paramter_vals_bigram(\n",
    "        unigram_laplace, bigram_laplace, bigram_lambda, pos_prior)\n",
    "\n",
    "    # unigram\n",
    "    positiveWordMap = generateWordDict(train_set, train_labels, 1)\n",
    "    negativeWordMap = generateWordDict(train_set, train_labels, 0)\n",
    "\n",
    "    positiveProbMap, positive_unk_prob = calculateProb(positiveWordMap, unigram_laplace)\n",
    "    negativeProbMap, negative_unk_prob = calculateProb(negativeWordMap, unigram_laplace)\n",
    "\n",
    "    pos_uni_score = []\n",
    "    neg_uni_score = []\n",
    "    for doc in tqdm(dev_set, disable=silently):\n",
    "        pos_prob = math.log(pos_prior)\n",
    "        neg_prob = math.log(1-pos_prior)\n",
    "\n",
    "        for word in doc:\n",
    "            if word in positiveProbMap:\n",
    "                pos_prob += math.log(positiveProbMap[word])\n",
    "            else:\n",
    "                pos_prob += math.log(positive_unk_prob)\n",
    "\n",
    "            if word in negativeProbMap:\n",
    "                neg_prob += math.log(negativeProbMap[word])\n",
    "            else:\n",
    "                neg_prob += math.log(negative_unk_prob)\n",
    "        pos_uni_score.append(pos_prob)\n",
    "        neg_uni_score.append(neg_prob)\n",
    "\n",
    "    # bigram\n",
    "    positiveBigramMap = generateBigramDict(train_set, train_labels, 1)\n",
    "    negativeBigramMap = generateBigramDict(train_set, train_labels, 0)\n",
    "\n",
    "    positiveBigramProbMap, positive_bigram_unk_prob = calculateProb(positiveBigramMap, bigram_laplace)\n",
    "    negativeBigramProbMap, negative_bigram_unk_prob = calculateProb(negativeBigramMap, bigram_laplace)\n",
    "    \n",
    "    yhats = []\n",
    "    pos_bi_score = []\n",
    "    neg_bi_score = []\n",
    "    \n",
    "    for doc in tqdm(dev_set, disable=silently):\n",
    "        pos_prob = math.log(pos_prior)\n",
    "        neg_prob = math.log(1-pos_prior)\n",
    "        \n",
    "        for j in range(len(doc)-1):\n",
    "            bigram = (doc[j], doc[j+1])\n",
    "            if bigram in positiveBigramProbMap:\n",
    "                pos_prob += math.log(positiveBigramProbMap[bigram])\n",
    "            else:\n",
    "                pos_prob += math.log(positive_bigram_unk_prob)\n",
    "\n",
    "            if bigram in negativeBigramProbMap:\n",
    "                neg_prob += math.log(negativeBigramProbMap[bigram])\n",
    "            else:\n",
    "                neg_prob += math.log(negative_bigram_unk_prob)\n",
    "        pos_bi_score.append(pos_prob)\n",
    "        neg_bi_score.append(neg_prob)\n",
    "\n",
    "    # mixture\n",
    "    for i in range(len(pos_uni_score)):\n",
    "        pos_prob = (1-bigram_lambda)*pos_uni_score[i] + (bigram_lambda)*pos_bi_score[i]\n",
    "        neg_prob = (1-bigram_lambda)*neg_uni_score[i] + (bigram_lambda)*neg_bi_score[i]\n",
    "        if (pos_prob >= neg_prob):\n",
    "            yhats.append(1)\n",
    "        else:\n",
    "            yhats.append(0)\n",
    "\n",
    "    return yhats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3189cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracies(predicted_labels, test_labels):\n",
    "    yhats = predicted_labels\n",
    "    assert len(yhats) == len(test_labels), \"predicted and gold label lists have different lengths\"\n",
    "    accuracy = sum([yhats[i] == test_labels[i] for i in range(len(yhats))]) / len(yhats)\n",
    "    tp = sum([yhats[i] == test_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    tn = sum([yhats[i] == test_labels[i] and yhats[i] == 0 for i in range(len(yhats))])\n",
    "    fp = sum([yhats[i] != test_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    fn = sum([yhats[i] != test_labels[i] and yhats[i] == 0 for i in range(len(yhats))])\n",
    "    return accuracy, fp, fn, tp, tn\n",
    "\n",
    "# print value and also percentage out of n\n",
    "def print_value(label, value, numvalues):\n",
    "   print(f\"{label} {value} ({value/numvalues}%)\")\n",
    "\n",
    "# print out performance stats\n",
    "def print_stats(accuracy, false_positive, false_negative, true_positive, true_negative, numvalues, classifierName):\n",
    "    print(classifierName + \" classifier accuracy:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print_value(\"False Positive\", false_positive,numvalues)\n",
    "    print_value(\"False Negative\", false_negative,numvalues)\n",
    "    print_value(\"True Positive\", true_positive,numvalues)\n",
    "    print_value(\"True Negative\", true_negative,numvalues)\n",
    "    print(f\"total number of samples {numvalues}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "267bf209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Laplace 0.001\n",
      "Bigram Laplace 0.005\n",
      "Bigram Lambda 0.5\n",
      "Positive prior 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 64516.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 50251.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Laplace 0.01\n",
      "Positive prior 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 66225.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Bayes classifier accuracy:\n",
      "Accuracy: 0.643\n",
      "False Positive 1578 (0.1578%)\n",
      "False Negative 1992 (0.1992%)\n",
      "True Positive 3016 (0.3016%)\n",
      "True Negative 3414 (0.3414%)\n",
      "total number of samples 10000\n",
      "Naive Bayes classifier accuracy:\n",
      "Accuracy: 0.5578\n",
      "False Positive 1846 (0.1846%)\n",
      "False Negative 2576 (0.2576%)\n",
      "True Positive 2432 (0.2432%)\n",
      "True Negative 3146 (0.3146%)\n",
      "total number of samples 10000\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_bigram = bigramBayes(X_train, y_train, X_test)\n",
    "predicted_labels_naive = naiveBayes(X_train, y_train, X_test)\n",
    "\n",
    "accuracy, false_positive, false_negative, true_positive, true_negative = compute_accuracies(predicted_labels_bigram,y_test)\n",
    "nn = len(y_test)\n",
    "print_stats(accuracy, false_positive, false_negative, true_positive, true_negative, nn, \"Bigram Bayes\")\n",
    "\n",
    "accuracy, false_positive, false_negative, true_positive, true_negative = compute_accuracies(predicted_labels_naive,y_test)\n",
    "print_stats(accuracy, false_positive, false_negative, true_positive, true_negative, nn, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f12f53e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [lamest, tweet, possible, friday, edition, toe...\n",
       "1          [indeed, sad, also, prayer, people, family]\n",
       "2                        [love, color, still, untried]\n",
       "3    [getting, ready, another, night, tent, 1st, ni...\n",
       "4                  [ugh, insomnia, one, keep, company]\n",
       "Name: processed_tweets, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data =  shuffle(data).reset_index(drop=True)\n",
    "\n",
    "# Tokenization\n",
    "tokenized_data=data['processed_tweets'].apply(lambda x: x.split())\n",
    "tokenized_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84d8a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(data['processed_tweets'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "449eb5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=text_counts\n",
    "y=data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8c73c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation score =  [0.7311 0.7306 0.7386 0.7313 0.7317]\n",
      "Train accuracy =85.02%\n",
      "Test accuracy =73.37%\n"
     ]
    }
   ],
   "source": [
    "# Complement Naive Bayes\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from math import *\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train, y_train)\n",
    "cross_cnb = cross_val_score(cnb, X, y,n_jobs = -1)\n",
    "print(\"Cross Validation score = \",cross_cnb)                \n",
    "print (\"Train accuracy ={:.2f}%\".format(cnb.score(X_train,y_train)*100))\n",
    "print (\"Test accuracy ={:.2f}%\".format(cnb.score(X_test,y_test)*100))\n",
    "train_acc_cnb=cnb.score(X_train,y_train)\n",
    "test_acc_cnb=cnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d574fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yh = cnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6282961b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complement Naive Bayes classifier accuracy:\n",
      "Accuracy: 0.7337\n",
      "False Positive 1208 (0.1208%)\n",
      "False Negative 1455 (0.1455%)\n",
      "True Positive 3583 (0.3583%)\n",
      "True Negative 3754 (0.3754%)\n",
      "total number of samples 10000\n"
     ]
    }
   ],
   "source": [
    "accuracy, false_positive, false_negative, true_positive, true_negative = compute_accuracies(yh.tolist(),y_test.tolist())\n",
    "nn = len(y_test)\n",
    "print_stats(accuracy, false_positive, false_negative, true_positive, true_negative, nn, \"Complement Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9735948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe71ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6eb5891f25db7a8f9d5f93b0192c288ba2fbe3f4f69cd0dd94ef07104d333ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
